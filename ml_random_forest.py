# -*- coding: utf-8 -*-
"""ML_random_forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gHd6TI-ECiEiZ3Qa-Dulwe_xRVxtrzyx
"""

#data handling
import pandas as pd
import numpy as np

#data visualization
import matplotlib.pyplot as plt
import seaborn as sns

#preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import label_binarize
from sklearn.preprocessing import MinMaxScaler

#feature selection
from sklearn.feature_selection import mutual_info_classif

#classification
from sklearn.multiclass import OneVsRestClassifier
from sklearn.ensemble import RandomForestClassifier

# performance metrics
from sklearn.metrics import balanced_accuracy_score,f1_score,precision_score, recall_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve,auc
from sklearn.metrics import roc_auc_score

# Load your dataset
transposed_data = pd.read_csv("transposed_data.csv")
print(transposed_data.head())

print(transposed_data["Description"].value_counts())

transposed_data["Description"].value_counts().plot.bar()

X =transposed_data.drop('Description', axis=1)
y = transposed_data['Description']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Predict on the test set
y_pred = rf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
#precision
precision=np.round(precision_score(y_test,y_pred,average = 'weighted'),4)
print('precision:%0.4f'%precision)

#recall
recall=np.round(recall_score(y_test,y_pred,average = 'weighted'),4)
print('recall:%0.4f'%recall)

#f1score
f1score=np.round(f1_score(y_test,y_pred,average = 'weighted'),4)
print('f1score:%0.4f'%f1score)

# Get feature importance
feature_importances = rf.feature_importances_

# Create a bar plot for feature importance
plt.figure(figsize=(10, 6))
plt.barh(X.columns, feature_importances)
plt.xlabel("Feature Importance")
plt.ylabel("Genes")
plt.title("Feature Importance in Random Forest Model")
plt.show()

# Get feature importance
feature_importances = rf.feature_importances_

# Create a DataFrame to hold features and their importance
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
})

# Sort features by importance in descending order
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Select the top 100 features
top_100_features = importance_df.head(100)['Feature']

# Filter the dataset to include only the top 100 features
X_top_100 = X[top_100_features]

print("Top 100 features selected:")
print(top_100_features)